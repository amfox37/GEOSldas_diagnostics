{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36ebd51-45bc-4267-9309-d2290ff7480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:14:26 | INFO | === Pipeline start ===\n",
      "15:14:26 | INFO | [LS_OLv8_M36] in-range files: 943 / hits: 8766\n",
      "15:14:26 | INFO | [LS_OLv8_M36] time span: 2000-06-01T12:00:00.000000000 … 2002-12-30T12:00:00.000000000\n",
      "15:14:27 | INFO | [LS_DAv8_M36] in-range files: 943 / hits: 8766\n",
      "15:14:27 | INFO | [LS_DAv8_M36] time span: 2000-06-01T12:00:00.000000000 … 2002-12-30T12:00:00.000000000\n",
      "15:14:27 | INFO | Years with both OL & DA: 2000–2002 (3 years)\n",
      "15:14:27 | INFO | [open] 1 files …\n",
      "15:14:27 | INFO | [open] done in 0.1s\n",
      "15:14:27 | INFO | === PASS A: valid-day keep mask ===\n",
      "15:14:27 | INFO | [open] 60 files …\n",
      "15:14:31 | INFO | [open] done in 4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.90 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:14:42 | INFO | [open] 60 files …\n",
      "15:14:47 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:14:59 | INFO | [open] 60 files …\n",
      "15:15:03 | INFO | [open] done in 4.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 9.87 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:14 | INFO | [open] 34 files …\n",
      "15:15:16 | INFO | [open] done in 2.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 4.81 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:21 | INFO | [open] 60 files …\n",
      "15:15:26 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 12.17 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:39 | INFO | [open] 60 files …\n",
      "15:15:43 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:55 | INFO | [open] 60 files …\n",
      "15:16:00 | INFO | [open] done in 4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.62 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:12 | INFO | [open] 34 files …\n",
      "15:16:15 | INFO | [open] done in 2.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 6.57 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:22 | INFO | [A] 2000 done.\n",
      "15:16:22 | INFO | [open] 60 files …\n",
      "15:16:26 | INFO | [open] done in 4.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.52 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:37 | INFO | [open] 60 files …\n",
      "15:16:42 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 9.93 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:52 | INFO | [open] 60 files …\n",
      "15:16:57 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 8.54 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:06 | INFO | [open] 60 files …\n",
      "15:17:10 | INFO | [open] done in 4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 7.68 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:19 | INFO | [open] 60 files …\n",
      "15:17:23 | INFO | [open] done in 4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 8.82 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:32 | INFO | [open] 60 files …\n",
      "15:17:37 | INFO | [open] done in 4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 9.80 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:48 | INFO | [open] 5 files …\n",
      "15:17:48 | INFO | [open] done in 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 715.79 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:49 | INFO | [open] 60 files …\n",
      "15:17:53 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:18:06 | INFO | [open] 60 files …\n",
      "15:18:10 | INFO | [open] done in 4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.73 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:18:23 | INFO | [open] 60 files …\n",
      "15:18:27 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.30 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:18:39 | INFO | [open] 60 files …\n",
      "15:18:44 | INFO | [open] done in 4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:18:56 | INFO | [open] 60 files …\n",
      "15:19:01 | INFO | [open] done in 4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:19:12 | INFO | [open] 60 files …\n",
      "15:19:17 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.97 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:19:29 | INFO | [open] 5 files …\n",
      "15:19:29 | INFO | [open] done in 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 1.12 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:19:30 | INFO | [A] 2001 done.\n",
      "15:19:30 | INFO | [open] 60 files …\n",
      "15:19:35 | INFO | [open] done in 5.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 12.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:19:48 | INFO | [open] 60 files …\n",
      "15:19:53 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.42 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:20:04 | INFO | [open] 60 files …\n",
      "15:20:08 | INFO | [open] done in 4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:20:19 | INFO | [open] 60 files …\n",
      "15:20:24 | INFO | [open] done in 4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:20:36 | INFO | [open] 60 files …\n",
      "15:20:41 | INFO | [open] done in 4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:20:52 | INFO | [open] 60 files …\n",
      "15:20:57 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:08 | INFO | [open] 4 files …\n",
      "15:21:09 | INFO | [open] done in 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 975.33 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:10 | INFO | [open] 60 files …\n",
      "15:21:14 | INFO | [open] done in 4.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.98 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:25 | INFO | [open] 60 files …\n",
      "15:21:30 | INFO | [open] done in 4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.30 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:42 | INFO | [open] 60 files …\n",
      "15:21:46 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.66 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:59 | INFO | [open] 60 files …\n",
      "15:22:03 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:15 | INFO | [open] 60 files …\n",
      "15:22:20 | INFO | [open] done in 5.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:33 | INFO | [open] 60 files …\n",
      "15:22:38 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:50 | INFO | [open] 4 files …\n",
      "15:22:50 | INFO | [open] done in 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 814.98 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:51 | INFO | [A] 2002 done.\n",
      "15:22:51 | INFO | [A] keep tiles: 72686/112573\n",
      "15:22:51 | INFO | → Writing ./yearly_outputs/keep_tile.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 100.79 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:51 | INFO | ✓ Wrote ./yearly_outputs/keep_tile.nc\n",
      "15:22:51 | INFO | === PASS B: OL climatology (DOY) ===\n",
      "15:22:51 | INFO | [B] 2000\n",
      "15:22:51 | INFO | [open] 60 files …\n",
      "15:22:56 | INFO | [open] done in 4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.66 s\n",
      "[########################################] | 100% Completed | 6.67 ss\n",
      "[########################################] | 100% Completed | 7.87 ss\n",
      "[########################################] | 100% Completed | 6.57 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:23:34 | INFO | [open] 60 files …\n",
      "15:23:39 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.15 s\n",
      "[########################################] | 100% Completed | 7.01 sms\n",
      "[########################################] | 100% Completed | 7.84 ss\n",
      "[########################################] | 100% Completed | 7.19 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:24:18 | INFO | [open] 60 files …\n",
      "15:24:23 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.25 s\n",
      "[########################################] | 100% Completed | 6.84 ss\n",
      "[########################################] | 100% Completed | 7.66 ss\n",
      "[########################################] | 100% Completed | 6.88 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:25:01 | INFO | [open] 34 files …\n",
      "15:25:03 | INFO | [open] done in 2.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 6.96 sms\n",
      "[########################################] | 100% Completed | 5.09 ss\n",
      "[########################################] | 100% Completed | 5.54 ss\n",
      "[########################################] | 100% Completed | 5.20 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:25:31 | INFO | [B] 2001\n",
      "15:25:31 | INFO | [open] 60 files …\n",
      "15:25:36 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.12 s\n",
      "[########################################] | 100% Completed | 6.85 ss\n",
      "[########################################] | 100% Completed | 7.90 ss\n",
      "[########################################] | 100% Completed | 6.78 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:26:15 | INFO | [open] 60 files …\n",
      "15:26:20 | INFO | [open] done in 5.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.07 s\n",
      "[########################################] | 100% Completed | 6.75 ss\n",
      "[########################################] | 100% Completed | 7.80 ss\n",
      "[########################################] | 100% Completed | 6.70 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:26:59 | INFO | [open] 60 files …\n",
      "15:27:04 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.80 s\n",
      "[########################################] | 100% Completed | 6.78 ss\n",
      "[########################################] | 100% Completed | 8.16 ss\n",
      "[########################################] | 100% Completed | 6.84 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:27:44 | INFO | [open] 60 files …\n",
      "15:27:49 | INFO | [open] done in 4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.01 s\n",
      "[########################################] | 100% Completed | 6.54 sms\n",
      "[########################################] | 100% Completed | 8.02 ss\n",
      "[########################################] | 100% Completed | 6.84 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:28:28 | INFO | [open] 60 files …\n",
      "15:28:33 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.87 s\n",
      "[########################################] | 100% Completed | 6.92 sms\n",
      "[########################################] | 100% Completed | 7.76 ss\n",
      "[########################################] | 100% Completed | 6.88 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:29:12 | INFO | [open] 60 files …\n",
      "15:29:16 | INFO | [open] done in 4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 12.47 s\n",
      "[########################################] | 100% Completed | 6.44 sms\n",
      "[########################################] | 100% Completed | 7.75 ss\n",
      "[########################################] | 100% Completed | 6.87 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:29:56 | INFO | [open] 5 files …\n",
      "15:29:57 | INFO | [open] done in 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 3.42 sms\n",
      "[########################################] | 100% Completed | 3.26 sms\n",
      "[########################################] | 100% Completed | 3.00 sms\n",
      "[########################################] | 100% Completed | 2.89 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:30:12 | INFO | [B] 2002\n",
      "15:30:12 | INFO | [open] 60 files …\n",
      "15:30:17 | INFO | [open] done in 4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.03 s\n",
      "[########################################] | 100% Completed | 6.44 sms\n",
      "[########################################] | 100% Completed | 8.11 ss\n",
      "[########################################] | 100% Completed | 6.86 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:30:56 | INFO | [open] 60 files …\n",
      "15:31:01 | INFO | [open] done in 4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.15 s\n",
      "[########################################] | 100% Completed | 6.76 sms\n",
      "[########################################] | 100% Completed | 7.62 ss\n",
      "[########################################] | 100% Completed | 6.78 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:31:39 | INFO | [open] 60 files …\n",
      "15:31:44 | INFO | [open] done in 5.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.02 s\n",
      "[########################################] | 100% Completed | 6.45 sms\n",
      "[########################################] | 100% Completed | 7.96 ss\n",
      "[########################################] | 100% Completed | 6.68 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:32:23 | INFO | [open] 60 files …\n",
      "15:32:28 | INFO | [open] done in 5.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.56 s\n",
      "[########################################] | 100% Completed | 7.02 sms\n",
      "[########################################] | 100% Completed | 8.26 ss\n",
      "[########################################] | 100% Completed | 6.94 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:33:08 | INFO | [open] 60 files …\n",
      "15:33:12 | INFO | [open] done in 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10.74 s\n",
      "[########################################] | 100% Completed | 6.56 sms\n",
      "[########################################] | 100% Completed | 8.20 ss\n",
      "[########################################] | 100% Completed | 7.05 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:33:51 | INFO | [open] 60 files …\n",
      "15:33:56 | INFO | [open] done in 4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11.07 s\n",
      "[########################################] | 100% Completed | 6.75 ss\n",
      "[########################################] | 100% Completed | 7.96 ss\n",
      "[########################################] | 100% Completed | 6.88 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34:35 | INFO | [open] 4 files …\n",
      "15:34:36 | INFO | [open] done in 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 3.45 ss\n",
      "[########################################] | 100% Completed | 2.97 sms\n",
      "[########################################] | 100% Completed | 2.99 sms\n",
      "[########################################] | 100% Completed | 2.98 sms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34:54 | INFO | → Writing ./yearly_outputs/OLv8_climatology_DOY_smooth_kept.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 101.33 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34:54 | INFO | ✓ Wrote ./yearly_outputs/OLv8_climatology_DOY_smooth_kept.nc\n",
      "15:34:54 | INFO | === PASS C: yearly anomalies (OL & DA) ===\n",
      "15:34:54 | INFO | [C] 2000\n",
      "15:34:54 | INFO | [open] 214 files …\n",
      "15:35:09 | INFO | [open] done in 15.1s\n",
      "15:35:10 | INFO | [open] 214 files …\n",
      "15:35:26 | INFO | [open] done in 15.7s\n",
      "15:35:29 | INFO | → Writing ./yearly_outputs/OLv8_daily_anomalies_kept_2000.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 476.38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:43:29 | INFO | ✓ Wrote ./yearly_outputs/OLv8_daily_anomalies_kept_2000.nc\n",
      "15:43:29 | INFO | → Writing ./yearly_outputs/DAv8_daily_anomalies_kept_2000.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 480.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:51:32 | INFO | ✓ Wrote ./yearly_outputs/DAv8_daily_anomalies_kept_2000.nc\n",
      "15:51:32 | INFO | [C] 2001\n",
      "15:51:32 | INFO | [open] 365 files …\n",
      "15:52:01 | INFO | [open] done in 29.1s\n",
      "15:52:03 | INFO | [open] 365 files …\n",
      "15:52:30 | INFO | [open] done in 27.4s\n",
      "15:52:34 | INFO | → Writing ./yearly_outputs/OLv8_daily_anomalies_kept_2001.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13m 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06:23 | INFO | ✓ Wrote ./yearly_outputs/OLv8_daily_anomalies_kept_2001.nc\n",
      "16:06:23 | INFO | → Writing ./yearly_outputs/DAv8_daily_anomalies_kept_2001.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13m 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:20:25 | INFO | ✓ Wrote ./yearly_outputs/DAv8_daily_anomalies_kept_2001.nc\n",
      "16:20:25 | INFO | [C] 2002\n",
      "16:20:25 | INFO | [open] 364 files …\n",
      "16:20:56 | INFO | [open] done in 30.1s\n",
      "16:20:58 | INFO | [open] 364 files …\n",
      "16:21:26 | INFO | [open] done in 28.7s\n",
      "16:21:32 | INFO | → Writing ./yearly_outputs/OLv8_daily_anomalies_kept_2002.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 14m 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:35:58 | INFO | ✓ Wrote ./yearly_outputs/OLv8_daily_anomalies_kept_2002.nc\n",
      "16:35:58 | INFO | → Writing ./yearly_outputs/DAv8_daily_anomalies_kept_2002.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13m 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:50:04 | INFO | ✓ Wrote ./yearly_outputs/DAv8_daily_anomalies_kept_2002.nc\n",
      "16:50:04 | INFO | === Done ===\n",
      "16:50:04 | INFO |  keep_tile: ./yearly_outputs/keep_tile.nc\n",
      "16:50:04 | INFO |  climatology: ./yearly_outputs/OLv8_climatology_DOY_smooth_kept.nc\n",
      "16:50:04 | INFO |  anomalies: ./yearly_outputs/OLv8_daily_anomalies_kept_YYYY.nc & DAv8_daily_anomalies_kept_YYYY.nc\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Annual-batched daily OL/DA pipeline (streaming & memory-safe, simplified):\n",
    "\n",
    "Pass A:\n",
    "  - Build a global keep_tile mask requiring ≥ MIN_VALID_FRAC valid days for BOTH OL and DA.\n",
    "\n",
    "Pass B:\n",
    "  - Build a global OL (CNTL) daily climatology (DOY=1..366) over kept tiles, then apply cyclic smoothing.\n",
    "\n",
    "Pass C:\n",
    "  - Write per-year OL/DA anomaly files using the global climatology and keep_tile subset.\n",
    "\n",
    "Outputs (in --outdir):\n",
    "  - keep_tile.nc\n",
    "  - OLv8_climatology_DOY_smooth_kept.nc\n",
    "  - OLv8_daily_anomalies_kept_YYYY.nc\n",
    "  - DAv8_daily_anomalies_kept_YYYY.nc\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, argparse, logging\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.utils import SerializableLock\n",
    "\n",
    "# -----------------------------\n",
    "# Basic config / tunables\n",
    "# -----------------------------\n",
    "DEFAULT_READ_ENGINE  = \"netcdf4\"\n",
    "DEFAULT_WRITE_ENGINE = \"h5netcdf\"\n",
    "_DEFAULT_CHUNKS = {\"time\": 31, \"tile\": 16384}\n",
    "\n",
    "TEMP_THRESH_K   = 275.15   # 2 °C\n",
    "SNOW_EPS        = 1e-2     # 1% snow cover\n",
    "MIN_VALID_FRAC  = 0.7\n",
    "_TS_RE = re.compile(r\"\\.(\\d{8})_1200z\\.nc4$\")   # ...YYYYMMDD_1200z.nc4\n",
    "\n",
    "# Dask setup: threads play nicer with HDF5\n",
    "_H5_LOCK = SerializableLock()\n",
    "dask.config.set({\"scheduler\": \"threads\",\n",
    "                 \"array.slicing.split_large_chunks\": True,\n",
    "                 \"optimization.fuse.active\": True})\n",
    "os.environ.setdefault(\"HDF5_USE_FILE_LOCKING\", \"FALSE\")\n",
    "\n",
    "# -----------------------------\n",
    "# Logging\n",
    "# -----------------------------\n",
    "def setup_logger(verbosity: int = 1) -> logging.Logger:\n",
    "    level = logging.INFO if verbosity == 1 else (logging.DEBUG if verbosity >= 2 else logging.WARNING)\n",
    "    logging.basicConfig(level=level, format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "                        datefmt=\"%H:%M:%S\", force=True)\n",
    "    return logging.getLogger(\"daily-da-yearly\")\n",
    "\n",
    "def stamp(log, msg): log.info(msg)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: discovery & opening\n",
    "# -----------------------------\n",
    "def _parse_ts(bname: str):\n",
    "    m = _TS_RE.search(bname)\n",
    "    if not m: return None\n",
    "    ymd = m.group(1)\n",
    "    return np.datetime64(f\"{ymd[:4]}-{ymd[4:6]}-{ymd[6:8]}T12:00\")\n",
    "\n",
    "def collect_daily_files(root_dir: str, file_prefix: str, start_date, end_date, log=None):\n",
    "    pattern = os.path.join(root_dir, \"**\", f\"{file_prefix}.tavg24_1d_lnd_Nt.*_1200z.nc4\")\n",
    "    hits = glob.glob(pattern, recursive=True)\n",
    "    if not hits: raise FileNotFoundError(f\"No *_1200z.nc4 under {root_dir} for {file_prefix}\")\n",
    "    start = np.datetime64(str(start_date), \"ns\"); end = np.datetime64(str(end_date), \"ns\")\n",
    "    files, times = [], []\n",
    "    for p in hits:\n",
    "        ts = _parse_ts(os.path.basename(p))\n",
    "        if ts is not None and start <= ts <= end:\n",
    "            files.append(p); times.append(ts)\n",
    "    if not files: raise FileNotFoundError(f\"No daily files for {file_prefix} within [{start_date}..{end_date}]\")\n",
    "    order = np.argsort(np.asarray(times))\n",
    "    files = [files[i] for i in order]\n",
    "    times = np.asarray(times, dtype=\"datetime64[ns]\")[order]\n",
    "    if log:\n",
    "        stamp(log, f\"[{file_prefix}] in-range files: {len(files)} / hits: {len(hits)}\")\n",
    "        stamp(log, f\"[{file_prefix}] time span: {str(times[0])} … {str(times[-1])}\")\n",
    "    return files, times\n",
    "\n",
    "def split_by_year(files, times):\n",
    "    years = np.array([int(str(t)[:4]) for t in times])\n",
    "    out = {}\n",
    "    for y in np.unique(years):\n",
    "        idx = np.where(years == y)[0]\n",
    "        out[y] = ([files[i] for i in idx], times[idx])\n",
    "    return out\n",
    "\n",
    "def speed_open_mfdataset(files, varnames, engine=\"netcdf4\", chunks=None, log=None):\n",
    "    if chunks is None: chunks = _DEFAULT_CHUNKS\n",
    "    want = set(varnames)\n",
    "    def _pre(ds):\n",
    "        keep = [v for v in ds.variables if v in want]\n",
    "        if not keep: raise KeyError(f\"Requested vars missing. Asked: {varnames}\")\n",
    "        return ds[keep]\n",
    "    if log: stamp(log, f\"[open] {len(files)} files …\")\n",
    "    t0 = time.perf_counter()\n",
    "    ds = xr.open_mfdataset(files, combine=\"nested\", concat_dim=\"time\", preprocess=_pre,\n",
    "                           engine=engine, parallel=False, lock=_H5_LOCK, chunks=chunks,\n",
    "                           data_vars=\"minimal\", coords=\"minimal\", compat=\"override\",\n",
    "                           mask_and_scale=False, decode_times=False, decode_coords=False,\n",
    "                           use_cftime=False)\n",
    "    if log: stamp(log, f\"[open] done in {time.perf_counter()-t0:.1f}s\")\n",
    "    return ds\n",
    "\n",
    "def _open_first_for_latlon(path: str, engine: str = \"netcdf4\"):\n",
    "    with xr.open_dataset(path, engine=engine, chunks={}) as ds0:\n",
    "        lat = ds0[\"lat\"].values; lon = ds0[\"lon\"].values\n",
    "    return lat, lon\n",
    "\n",
    "def batched(seq, n):\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i+n], slice(i, min(i+n, len(seq)))\n",
    "\n",
    "# -----------------------------\n",
    "# Masking & DOY\n",
    "# -----------------------------\n",
    "def apply_frozen_snow_mask(sm_da, tsoil, frsnow, temp_thresh=TEMP_THRESH_K, snow_eps=SNOW_EPS):\n",
    "    return sm_da.where(~((tsoil < temp_thresh) | (frsnow > snow_eps)))\n",
    "\n",
    "def mask_vars(ds: xr.Dataset, anom_vars, temp_k, snow_eps) -> xr.Dataset:\n",
    "    out = xr.Dataset(coords=ds.coords)\n",
    "    for v in anom_vars:\n",
    "        sm = ds[v]\n",
    "        tsoil  = ds[\"TSOIL1\"] if \"TSOIL1\" in ds else xr.full_like(sm, np.nan)\n",
    "        frsnow = ds[\"FRLANDSNO\"] if \"FRLANDSNO\" in ds else xr.full_like(sm, np.nan)\n",
    "        out[v] = apply_frozen_snow_mask(sm, tsoil, frsnow, temp_k, snow_eps)\n",
    "    return out\n",
    "\n",
    "def doy_index(da_time):\n",
    "    return xr.where(da_time.dt.dayofyear == 366, 365, da_time.dt.dayofyear)\n",
    "\n",
    "# -----------------------------\n",
    "# Write util\n",
    "# -----------------------------\n",
    "def write_nc(ds: xr.Dataset, path: str, engine: str, chunks=None, log=None):\n",
    "    if chunks is None: chunks = _DEFAULT_CHUNKS\n",
    "    tlen = int(ds.sizes.get(\"time\", 1)); ntiles = int(ds.sizes.get(\"tile\", 1))\n",
    "    time_chunk = min(tlen, int(chunks.get(\"time\", 31)))\n",
    "    tile_chunk = min(ntiles, int(chunks.get(\"tile\", 16384)))\n",
    "    comp = dict(zlib=True, complevel=4)\n",
    "    encoding = {}\n",
    "    for v in ds.data_vars:\n",
    "        dims = ds[v].dims\n",
    "        if dims == (\"time\", \"tile\"):\n",
    "            encoding[v] = {**comp, \"chunksizes\": (time_chunk, tile_chunk)}\n",
    "    if log: stamp(log, f\"→ Writing {path}\")\n",
    "    delayed = ds.to_netcdf(path, engine=engine, encoding=encoding, compute=False)\n",
    "    with ProgressBar(): dask.compute(delayed)\n",
    "    if log: stamp(log, f\"✓ Wrote {path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Arg parsing\n",
    "# -----------------------------\n",
    "def parse_chunk_flag(s: str) -> dict:\n",
    "    out = {}\n",
    "    if not s: return out\n",
    "    for kv in s.split(\",\"):\n",
    "        if not kv.strip(): continue\n",
    "        k, v = kv.split(\":\"); out[k.strip()] = int(v.strip())\n",
    "    return out\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Annual-batched OL/DA anomalies with global keep_tile and climatology (streaming).\")\n",
    "    # Roots/prefixes\n",
    "    p.add_argument(\"--ol-root\", required=False, default=\"/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg\")\n",
    "    p.add_argument(\"--ol-prefix\", default=\"LS_OLv8_M36\")\n",
    "    p.add_argument(\"--da-root\", required=False, default=\"/discover/nobackup/projects/land_da/M21C_land_sweeper/LS_DAv8_M36_v2/LS_DAv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg\")\n",
    "    p.add_argument(\"--da-prefix\", default=\"LS_DAv8_M36\")\n",
    "    # Vars\n",
    "    p.add_argument(\"--vars\", nargs=\"+\", default=[\"SFMC\",\"RZMC\",\"PRECTOTCORRLAND\",\"FRLANDSNO\",\"TSOIL1\"])\n",
    "    p.add_argument(\"--anom-vars\", nargs=\"+\", default=[\"SFMC\",\"RZMC\"])\n",
    "    # Time & I/O\n",
    "    p.add_argument(\"--start-date\", default=\"2000-01-01\")\n",
    "    p.add_argument(\"--end-date\",   default=\"2002-12-31\")\n",
    "    p.add_argument(\"--read-engine\", choices=[\"h5netcdf\",\"netcdf4\"], default=DEFAULT_READ_ENGINE)\n",
    "    p.add_argument(\"--write-engine\", choices=[\"h5netcdf\",\"netcdf4\"], default=DEFAULT_WRITE_ENGINE)\n",
    "    p.add_argument(\"--chunks\", default=\"time:31,tile:8192\")\n",
    "    p.add_argument(\"--outdir\", default=\"./yearly_outputs\")\n",
    "    # Mask/clim\n",
    "    p.add_argument(\"--temp-K\", type=float, default=TEMP_THRESH_K)\n",
    "    p.add_argument(\"--snow-eps\", type=float, default=SNOW_EPS)\n",
    "    p.add_argument(\"--min-valid-frac\", type=float, default=MIN_VALID_FRAC)\n",
    "    p.add_argument(\"--clim-window\", type=int, default=31)\n",
    "    # Batching\n",
    "    p.add_argument(\"--batch-days\", type=int, default=60, help=\"Days per file batch in streaming steps (45–90 good).\")\n",
    "    # Verbosity\n",
    "    p.add_argument(\"--verbose\", type=int, default=1)\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    log = setup_logger(args.verbose)\n",
    "    chunks = parse_chunk_flag(args.chunks)\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "    stamp(log, \"=== Pipeline start ===\")\n",
    "    files_ol, times_ol = collect_daily_files(args.ol_root, args.ol_prefix, args.start_date, args.end_date, log)\n",
    "    files_da, times_da = collect_daily_files(args.da_root, args.da_prefix, args.start_date, args.end_date, log)\n",
    "    yearmap_ol = split_by_year(files_ol, times_ol)\n",
    "    yearmap_da = split_by_year(files_da, times_da)\n",
    "    years = sorted(set(yearmap_ol.keys()) & set(yearmap_da.keys()))\n",
    "    stamp(log, f\"Years with both OL & DA: {years[0]}–{years[-1]} ({len(years)} years)\")\n",
    "\n",
    "    lat, lon = _open_first_for_latlon(yearmap_ol[years[0]][0][0], args.read_engine)\n",
    "\n",
    "    # Peek to get ntiles\n",
    "    one_file = [yearmap_ol[years[0]][0][0]]\n",
    "    tmp = speed_open_mfdataset(one_file, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "    ntiles = int(tmp.sizes[\"tile\"])\n",
    "    del tmp\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS A: keep_tile\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS A: valid-day keep mask ===\")\n",
    "    BATCH = max(1, int(args.batch_days))\n",
    "\n",
    "    def accumulate_valid_for_filebatch(files, times):\n",
    "        ds = speed_open_mfdataset(files, args.vars + [\"TSOIL1\",\"FRLANDSNO\"], engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds = ds.assign_coords({\"time\": (\"time\", times), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)})\n",
    "        val = None\n",
    "        for v in args.anom_vars:\n",
    "            sm = ds[v]\n",
    "            tsoil = ds[\"TSOIL1\"] if \"TSOIL1\" in ds else xr.full_like(sm, np.nan)\n",
    "            frsn  = ds[\"FRLANDSNO\"] if \"FRLANDSNO\" in ds else xr.full_like(sm, np.nan)\n",
    "            ok    = apply_frozen_snow_mask(sm, tsoil, frsn, args.temp_K, args.snow_eps).notnull()\n",
    "            val   = ok if val is None else (val & ok)\n",
    "        cnt = val.astype(\"int8\").sum(\"time\", dtype=\"int64\")\n",
    "        with ProgressBar():\n",
    "            return cnt.compute().values, int(val.sizes[\"time\"])\n",
    "\n",
    "    accum_valid_ol = np.zeros((ntiles,), dtype=\"int64\")\n",
    "    accum_valid_da = np.zeros((ntiles,), dtype=\"int64\")\n",
    "    accum_days_ol = 0; accum_days_da = 0\n",
    "\n",
    "    for y in years:\n",
    "        f_ol, t_ol = yearmap_ol[y]; f_da, t_da = yearmap_da[y]\n",
    "        for fbatch, s in batched(f_ol, BATCH):\n",
    "            tbatch = t_ol[s]; cnt, days = accumulate_valid_for_filebatch(fbatch, tbatch)\n",
    "            accum_valid_ol += cnt; accum_days_ol += days\n",
    "        for fbatch, s in batched(f_da, BATCH):\n",
    "            tbatch = t_da[s]; cnt, days = accumulate_valid_for_filebatch(fbatch, tbatch)\n",
    "            accum_valid_da += cnt; accum_days_da += days\n",
    "        stamp(log, f\"[A] {y} done.\")\n",
    "\n",
    "    frac_ol = accum_valid_ol / max(1, accum_days_ol)\n",
    "    frac_da = accum_valid_da / max(1, accum_days_da)\n",
    "    keep_tile = (frac_ol >= args.min_valid_frac) & (frac_da >= args.min_valid_frac)\n",
    "    keep_idx = np.where(keep_tile)[0]\n",
    "    stamp(log, f\"[A] keep tiles: {keep_tile.sum()}/{ntiles}\")\n",
    "    keep_ds = xr.Dataset(dict(keep_tile=xr.DataArray(keep_tile, dims=(\"tile\",),\n",
    "                         coords={\"tile\": np.arange(ntiles), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)})))\n",
    "    write_nc(keep_ds, os.path.join(args.outdir, \"keep_tile.nc\"), args.write_engine, {\"tile\": _DEFAULT_CHUNKS[\"tile\"]}, log)\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS B: climatology on kept tiles\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS B: OL climatology (DOY) ===\")\n",
    "    DOY = 366\n",
    "    clim_sums = {v: np.zeros((DOY, len(keep_idx)), dtype=np.float64) for v in args.anom_vars}\n",
    "    clim_cnts = {v: np.zeros((DOY, len(keep_idx)), dtype=np.int64)   for v in args.anom_vars}\n",
    "\n",
    "    def accumulate_doy_for_batch(files_batch, times_batch):\n",
    "        ds = speed_open_mfdataset(files_batch, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds = ds.assign_coords({\"time\": (\"time\", times_batch), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)})\n",
    "        olm = mask_vars(ds, args.anom_vars, args.temp_K, args.snow_eps).isel(tile=keep_idx)\n",
    "        doy = xr.where(olm.time.dt.dayofyear == 366, 365, olm.time.dt.dayofyear)  # map 366→365 for grouping\n",
    "        full = xr.DataArray(np.arange(1, 367), dims=\"dayofyear\", name=\"dayofyear\")\n",
    "        sums_b, cnts_b = {}, {}\n",
    "        for v in args.anom_vars:\n",
    "            g = olm[v].groupby(doy)\n",
    "            with ProgressBar():\n",
    "                s = g.sum(\"time\", skipna=True).reindex(dayofyear=full, fill_value=0.0).compute()\n",
    "                c = g.count(\"time\").reindex(dayofyear=full, fill_value=0).compute()\n",
    "            sums_b[v] = s.values  # (366, n_keep)\n",
    "            cnts_b[v] = c.values\n",
    "        return sums_b, cnts_b\n",
    "\n",
    "    for y in years:\n",
    "        f_ol, t_ol = yearmap_ol[y]\n",
    "        stamp(log, f\"[B] {y}\")\n",
    "        for fbatch, s in batched(f_ol, BATCH):\n",
    "            tbatch = t_ol[s]\n",
    "            sums_b, cnts_b = accumulate_doy_for_batch(fbatch, tbatch)\n",
    "            for v in args.anom_vars:\n",
    "                clim_sums[v] += sums_b[v]\n",
    "                clim_cnts[v] += cnts_b[v]\n",
    "\n",
    "    # If DOY=366 never observed, copy 365 over (avoid divide-by-zero and keep continuity)\n",
    "    for v in args.anom_vars:\n",
    "        zero_366 = clim_cnts[v][-1, :] == 0\n",
    "        clim_sums[v][-1, zero_366] = clim_sums[v][-2, zero_366]\n",
    "        clim_cnts[v][-1, zero_366] = clim_cnts[v][-2, zero_366]\n",
    "\n",
    "    clim_vars = {}\n",
    "    for v in args.anom_vars:\n",
    "        count = clim_cnts[v]\n",
    "        summ  = clim_sums[v]\n",
    "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "            clim = summ / np.maximum(count, 1)\n",
    "\n",
    "        # Cyclic smoothing that PRESERVES length (366) using circular padding + valid convolution\n",
    "        def cyclic_smooth_same_len(arr2d, window: int):\n",
    "            w = max(1, int(window))\n",
    "            if w == 1: return arr2d\n",
    "            left = w // 2\n",
    "            right = (w - 1) // 2\n",
    "            pad = np.pad(arr2d, ((left, right), (0,0)), mode=\"wrap\")\n",
    "            kernel = np.ones(w, dtype=np.float64) / w\n",
    "            # convolve each column separately (axis=0), mode='valid' → original length\n",
    "            out = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode=\"valid\"), 0, pad)\n",
    "            return out\n",
    "\n",
    "        clim = cyclic_smooth_same_len(clim, args.clim_window)\n",
    "        clim_vars[f\"{v}_clim\"] = clim  # (366, n_keep)\n",
    "\n",
    "    coords = dict(dayofyear=np.arange(1, DOY+1),\n",
    "                  tile=(\"tile\", keep_idx),\n",
    "                  lat=(\"tile\", lat[keep_idx]),\n",
    "                  lon=(\"tile\", lon[keep_idx]))\n",
    "    clim_ds = xr.Dataset({k: ((\"dayofyear\",\"tile\"), v) for k, v in clim_vars.items()}, coords=coords)\n",
    "    clim_ds.attrs.update(baseline=\"CNTL masked DOY mean (global, smoothed)\",\n",
    "                         smoothing=f\"cyclic {args.clim_window}-day\")\n",
    "    clim_path = os.path.join(args.outdir, \"OLv8_climatology_DOY_smooth_kept.nc\")\n",
    "    write_nc(clim_ds, clim_path, args.write_engine, {\"dayofyear\": DOY, \"tile\": _DEFAULT_CHUNKS[\"tile\"]}, log)\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS C: per-year anomalies\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS C: yearly anomalies (OL & DA) ===\")\n",
    "    for y in years:\n",
    "        stamp(log, f\"[C] {y}\")\n",
    "        f_ol, t_ol = yearmap_ol[y]; f_da, t_da = yearmap_da[y]\n",
    "\n",
    "        ds_ol_y = speed_open_mfdataset(f_ol, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds_ol_y = ds_ol_y.assign_coords({\"time\": (\"time\", t_ol), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)}).isel(tile=keep_idx)\n",
    "        ds_da_y = speed_open_mfdataset(f_da, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds_da_y = ds_da_y.assign_coords({\"time\": (\"time\", t_da), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)}).isel(tile=keep_idx)\n",
    "\n",
    "        olm = mask_vars(ds_ol_y, args.anom_vars, args.temp_K, args.snow_eps)\n",
    "        dam = mask_vars(ds_da_y, args.anom_vars, args.temp_K, args.snow_eps)\n",
    "\n",
    "        doy = doy_index(olm.time)\n",
    "        an_ol = xr.Dataset(coords=dict(time=olm.time, tile=(\"tile\", keep_idx),\n",
    "                                       lat=(\"tile\", lat[keep_idx]), lon=(\"tile\", lon[keep_idx])))\n",
    "        an_da = xr.Dataset(coords=dict(time=dam.time, tile=(\"tile\", keep_idx),\n",
    "                                       lat=(\"tile\", lat[keep_idx]), lon=(\"tile\", lon[keep_idx])))\n",
    "        for v in args.anom_vars:\n",
    "            base = clim_ds[f\"{v}_clim\"].sel(dayofyear=doy).transpose(\"time\",\"tile\")\n",
    "            an_ol[v] = olm[v] - base\n",
    "            an_da[v] = dam[v] - base\n",
    "\n",
    "        out_ol_y = os.path.join(args.outdir, f\"OLv8_daily_anomalies_kept_{y}.nc\")\n",
    "        out_da_y = os.path.join(args.outdir, f\"DAv8_daily_anomalies_kept_{y}.nc\")\n",
    "        write_nc(an_ol, out_ol_y, args.write_engine, chunks, log)\n",
    "        write_nc(an_da, out_da_y, args.write_engine, chunks, log)\n",
    "\n",
    "        del ds_ol_y, ds_da_y, olm, dam, an_ol, an_da\n",
    "\n",
    "    stamp(log, \"=== Done ===\")\n",
    "    stamp(log, f\" keep_tile: {os.path.join(args.outdir, 'keep_tile.nc')}\")\n",
    "    stamp(log, f\" climatology: {clim_path}\")\n",
    "    stamp(log, f\" anomalies: {args.outdir}/OLv8_daily_anomalies_kept_YYYY.nc & DAv8_daily_anomalies_kept_YYYY.nc\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83e583-496d-4717-bfb8-c8b5ec5e0fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2dcb3-83e0-4ca8-a2c0-0cd0a2e9d08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
