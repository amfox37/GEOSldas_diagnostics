{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54fe27b-0c64-45d8-aa97-ab32db88b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from mapper_functions import plot_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2252ee9a-3ac2-4723-bc9c-6ef8e0922e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'DAv7_M36_MULTI_type_13_comb_fp_scaled'\n",
    "\n",
    "start_date = datetime(2015, 4, 1)\n",
    "end_date = datetime(2021, 4, 1)\n",
    "\n",
    "start_date_str = start_date.strftime('%Y%m%d')\n",
    "end_date_str = end_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adbc728-ae57-401d-9794-4538488447c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of lon: (112573,)\n",
      "Dimensions of lat: (112573,)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the netCDF file\n",
    "file_path = f'/discover/nobackup/amfox/Experiments/{expt_name}/{expt_name}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2015/M04/{expt_name}.catch_progn_incr.20150402.nc4'\n",
    "\n",
    "# Open the netCDF file\n",
    "dataset = xr.open_dataset(file_path)\n",
    "\n",
    "# Extract the lon and lat variables\n",
    "lon = dataset['lon']\n",
    "lat = dataset['lat']\n",
    "\n",
    "# Print the dimensions of the variables\n",
    "print(f\"Dimensions of lon: {lon.shape}\")\n",
    "print(f\"Dimensions of lat: {lat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513aeb91-2265-408c-a5ea-e52cefa83c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each of the saved files\n",
    "\n",
    "data_2015 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2015.npz', allow_pickle=True)\n",
    "data_2016 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2016.npz', allow_pickle=True)\n",
    "data_2017 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2017.npz', allow_pickle=True)\n",
    "data_2018 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2018.npz', allow_pickle=True)\n",
    "data_2019 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2019.npz', allow_pickle=True)\n",
    "data_2020 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2020.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a04fe4-3702-4da2-ab6e-8b826a541c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and combine the data from each of the files\n",
    "date_time = np.concatenate((data_2015['date_time'], data_2016['date_time'], data_2017['date_time'], data_2018['date_time'], data_2019['date_time'], data_2020['date_time']))\n",
    "obs_species = np.concatenate((data_2015['obs_species'], data_2016['obs_species'], data_2017['obs_species'], data_2018['obs_species'], data_2019['obs_species'], data_2020['obs_species']))\n",
    "obs_tilenum = np.concatenate((data_2015['obs_tilenum'], data_2016['obs_tilenum'], data_2017['obs_tilenum'], data_2018['obs_tilenum'], data_2019['obs_tilenum'], data_2020['obs_tilenum']))\n",
    "obs_lon = np.concatenate((data_2015['obs_lon'], data_2016['obs_lon'], data_2017['obs_lon'], data_2018['obs_lon'], data_2019['obs_lon'], data_2020['obs_lon']))\n",
    "obs_lat = np.concatenate((data_2015['obs_lat'], data_2016['obs_lat'], data_2017['obs_lat'], data_2018['obs_lat'], data_2019['obs_lat'], data_2020['obs_lat']))\n",
    "obs_obs = np.concatenate((data_2015['obs_obs'], data_2016['obs_obs'], data_2017['obs_obs'], data_2018['obs_obs'], data_2019['obs_obs'], data_2020['obs_obs']))\n",
    "obs_fcst = np.concatenate((data_2015['obs_fcst'], data_2016['obs_fcst'], data_2017['obs_fcst'], data_2018['obs_fcst'], data_2019['obs_fcst'], data_2020['obs_fcst']))\n",
    "obs_ana = np.concatenate((data_2015['obs_ana'], data_2016['obs_ana'], data_2017['obs_ana'], data_2018['obs_ana'], data_2019['obs_ana'], data_2020['obs_ana']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2af0db-bec8-4b4b-94aa-8e32e6669e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_2015 = np.load(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_extend_datetime_2015.npz', allow_pickle=True)\n",
    "\n",
    "## Extract and combine the data from each of the files\n",
    "#date_time = ((data_2015['date_time']))\n",
    "#obs_species = ((data_2015['obs_species']))\n",
    "#obs_tilenum = ((data_2015['obs_tilenum']))\n",
    "#obs_lon = ((data_2015['obs_lon']))\n",
    "#obs_lat = ((data_2015['obs_lat']))\n",
    "#obs_obs = ((data_2015['obs_obs']))\n",
    "#obs_fcst = ((data_2015['obs_fcst']))\n",
    "#obs_ana = ((data_2015['obs_ana']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcbaf13-abe8-4c22-9371-1f4ece9d1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate obs minus fcst\n",
    "obs_minus_fcst = []\n",
    "obs_minus_ana = []\n",
    "for i in range(len(obs_obs)):\n",
    "    obs_minus_fcst_chunk = obs_obs[i] - obs_fcst[i]\n",
    "    obs_minus_fcst.append(obs_minus_fcst_chunk)\n",
    "    obs_minus_ana_chunk = obs_obs[i] - obs_ana[i]\n",
    "    obs_minus_ana.append(obs_minus_ana_chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4b19b6-ff45-4c85-b1cb-f89baee471a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate obs minus ana\n",
    "# obs_minus_ana = []\n",
    "# for i in range(len(obs_obs)):\n",
    "#     obs_minus_ana_chunk = obs_obs[i] - obs_ana[i]\n",
    "#     obs_minus_ana.append(obs_minus_ana_chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973b55d0-ec23-4a42-9661-ea49dc2739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to numpy arrays\n",
    "obs_minus_fcst = np.array(obs_minus_fcst)\n",
    "obs_minus_ana = np.array(obs_minus_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c08734-e507-4a4c-a815-4bc811886d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tilenum values: 96049\n"
     ]
    }
   ],
   "source": [
    "# Find unique tilenum values\n",
    "unique_tilenum = np.unique(obs_tilenum)\n",
    "\n",
    "# Find the number of unique tilenum values\n",
    "num_unique_tilenum = len(unique_tilenum)\n",
    "\n",
    "# Print the number of unique tilenum values\n",
    "print(f\"Number of unique tilenum values: {num_unique_tilenum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074bab8c-7585-4ee8-a365-8061c2959e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the arrays based on obs_tilenum\n",
    "sort_indices = np.argsort(obs_tilenum)\n",
    "sorted_obs_tilenum = obs_tilenum[sort_indices]\n",
    "sorted_obs_species = obs_species[sort_indices]\n",
    "sorted_obs_obs = obs_obs[sort_indices]\n",
    "sorted_obs_fcst = obs_fcst[sort_indices]\n",
    "sorted_obs_ana = obs_ana[sort_indices]\n",
    "sorted_obs_minus_fcst = obs_minus_fcst[sort_indices]\n",
    "sorted_obs_minus_ana = obs_minus_ana[sort_indices]\n",
    "\n",
    "# Find the unique tilenum values and their counts\n",
    "unique_tilenum, counts = np.unique(sorted_obs_tilenum, return_counts=True)\n",
    "\n",
    "# Calculate the indices where the groups should be split\n",
    "split_indices = np.cumsum(counts)[:-1]\n",
    "\n",
    "# Split the sorted arrays based on the split indices\n",
    "obs_species_grouped = np.split(sorted_obs_species, split_indices)\n",
    "obs_obs_grouped = np.split(sorted_obs_obs, split_indices)\n",
    "obs_fcst_grouped = np.split(sorted_obs_fcst, split_indices)\n",
    "obs_ana_grouped = np.split(sorted_obs_ana, split_indices)\n",
    "obs_minus_fcst_grouped = np.split(sorted_obs_minus_fcst, split_indices)\n",
    "obs_minus_ana_grouped = np.split(sorted_obs_minus_ana, split_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a082d1ef-770b-46fe-bb62-997bb9fcd11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of obs_obs_grouped: 96049\n"
     ]
    }
   ],
   "source": [
    "# Print the length of obs_obs_grouped\n",
    "print(f\"Length of obs_obs_grouped: {len(obs_obs_grouped)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67dd12de-e763-4497-8483-2d67763aae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you have a multi-sensor experiment\n",
    "\n",
    "# Find the number of observations for each tilenum\n",
    "num_obs_smap = []\n",
    "num_obs_ascat = []\n",
    "num_obs_smap_cm = []\n",
    "num_obs_ascat_cm = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    num_obs_smap.append(len(obs_obs_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    num_obs_ascat.append(len(obs_obs_grouped[i][obs_species_grouped[i] > 4]))\n",
    "    # We need to cross mask the observations to get the observations if obs_obs_grouped[i] contains both SMAP and ASCAT observations\n",
    "    if len(obs_obs_grouped[i][obs_species_grouped[i] < 5]) > 0 and len(obs_obs_grouped[i][obs_species_grouped[i] > 4]) > 0:\n",
    "        num_obs_smap_cm.append(len(obs_obs_grouped[i][obs_species_grouped[i] < 5]))\n",
    "        num_obs_ascat_cm.append(len(obs_obs_grouped[i][obs_species_grouped[i] > 4]))\n",
    "    else:\n",
    "        num_obs_smap_cm.append(0)\n",
    "        num_obs_ascat_cm.append(0)\n",
    "        \n",
    "#Calculate the mean of the observations for each tilenum\n",
    "mean_obs_smap = []\n",
    "mean_obs_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    mean_obs_smap.append(np.mean(obs_obs_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    mean_obs_ascat.append(np.mean(obs_obs_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the mean of the forecasts for each tilenum\n",
    "mean_fcst_smap = []\n",
    "mean_fcst_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    mean_fcst_smap.append(np.mean(obs_fcst_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    mean_fcst_ascat.append(np.mean(obs_fcst_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the mean of the analyses for each tilenum\n",
    "mean_ana_smap = []\n",
    "mean_ana_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    mean_ana_smap.append(np.mean(obs_ana_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    mean_ana_ascat.append(np.mean(obs_ana_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the mean of the obs minus fcst for each tilenum\n",
    "mean_obs_minus_fcst_smap = []\n",
    "mean_obs_minus_fcst_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    mean_obs_minus_fcst_smap.append(np.mean(obs_minus_fcst_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    mean_obs_minus_fcst_ascat.append(np.mean(obs_minus_fcst_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the mean of the obs minus ana for each tilenum\n",
    "mean_obs_minus_ana_smap = []\n",
    "mean_obs_minus_ana_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    mean_obs_minus_ana_smap.append(np.mean(obs_minus_ana_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    mean_obs_minus_ana_ascat.append(np.mean(obs_minus_ana_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the standard deviation of the obs_minus_fcst for each tilenum\n",
    "std_obs_minus_fcst_smap = []\n",
    "std_obs_minus_fcst_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    std_obs_minus_fcst_smap.append(np.std(obs_minus_fcst_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    std_obs_minus_fcst_ascat.append(np.std(obs_minus_fcst_grouped[i][obs_species_grouped[i] > 4]))\n",
    "\n",
    "# Calculate the standard deviation of the obs_minus_ana for each tilenum\n",
    "std_obs_minus_ana_smap = []\n",
    "std_obs_minus_ana_ascat = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    std_obs_minus_ana_smap.append(np.std(obs_minus_ana_grouped[i][obs_species_grouped[i] < 5]))\n",
    "    std_obs_minus_ana_ascat.append(np.std(obs_minus_ana_grouped[i][obs_species_grouped[i] > 4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec282b-5dd0-417c-a6aa-32f0d5e54a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign lon and lat to each tilenum\n",
    "lon_tilenum = []\n",
    "lat_tilenum = []\n",
    "for i in range(num_unique_tilenum):\n",
    "    lon_tilenum.append(lon[int(unique_tilenum[i])])\n",
    "    lat_tilenum.append(lat[int(unique_tilenum[i])])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "lon_tilenum = np.array(lon_tilenum)\n",
    "lat_tilenum = np.array(lat_tilenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffaab8a-e552-4534-a281-5d7dcf1a4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you have a multi-sensor experiment\n",
    "# Save all the calculated values to a file\n",
    "np.savez(f'{expt_name}_{start_date_str}_{end_date_str}_obsfcstana_stats.npz',\n",
    "         unique_tilenum=unique_tilenum,\n",
    "         num_obs_smap=num_obs_smap,\n",
    "         num_obs_ascat=num_obs_ascat,\n",
    "         num_obs_smap_cm=num_obs_smap_cm,\n",
    "         num_obs_ascat_cm=num_obs_ascat_cm,\n",
    "         mean_obs_smap=mean_obs_smap,\n",
    "         mean_obs_ascat=mean_obs_ascat,\n",
    "         mean_fcst_smap=mean_fcst_smap,\n",
    "         mean_fcst_ascat=mean_fcst_ascat,\n",
    "         mean_ana_smap=mean_ana_smap,\n",
    "         mean_ana_ascat=mean_ana_ascat,\n",
    "         mean_obs_minus_fcst_smap=mean_obs_minus_fcst_smap,\n",
    "         mean_obs_minus_fcst_ascat=mean_obs_minus_fcst_ascat,\n",
    "         mean_obs_minus_ana_smap=mean_obs_minus_ana_smap,\n",
    "         mean_obs_minus_ana_ascat=mean_obs_minus_ana_ascat,\n",
    "         std_obs_minus_fcst_smap=std_obs_minus_fcst_smap,\n",
    "         std_obs_minus_fcst_ascat=std_obs_minus_fcst_ascat,\n",
    "         std_obs_minus_ana_smap=std_obs_minus_ana_smap,\n",
    "         std_obs_minus_ana_ascat=std_obs_minus_ana_ascat,\n",
    "         lon_tilenum=lon_tilenum,\n",
    "         lat_tilenum=lat_tilenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaab4cd-2ad8-479b-8be1-b3055888b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_array = num_obs_ascat\n",
    "test_array = np.array(test_array)\n",
    "test_array[test_array < 1] = -999\n",
    "    \n",
    "obarray = np.empty([num_unique_tilenum, 3])\n",
    "obarray[:, 1] = lon_tilenum\n",
    "obarray[:, 2] = lat_tilenum\n",
    "obarray[:, 0] = test_array\n",
    "   \n",
    "plot_global(obarray,False,'Number of ASCAT Obs Assimilated','Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd743-fb64-412d-956d-73dc595ce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "obarray[:, 0] = num_obs_smap_cm\n",
    "   \n",
    "plot_global(obarray,False,'num_obs_smap_cm','Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839133e-31d0-4b21-88b9-12e9b6618399",
   "metadata": {},
   "outputs": [],
   "source": [
    "obarray[:, 0] = num_obs_smap\n",
    "   \n",
    "plot_global(obarray,False,'num_obs_smap','Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606adb25-4944-468d-8586-dc33870c46cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
