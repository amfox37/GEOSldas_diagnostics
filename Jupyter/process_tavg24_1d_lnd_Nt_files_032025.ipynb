{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab9df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dask import compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a614ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the root directory and experiment name\n",
    "expt_name = 'LS_OLv8_M36'\n",
    "# root_directory = f'/discover/nobackup/amfox/Experiments/snow_M21C_test/{expt_name}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens0000'\n",
    "root_directory = f'/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/{expt_name}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2000, 10, 1)\n",
    "end_date = datetime(2023, 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076ca952-e2ec-4d88-8880-69621a6504ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/dask_jobqueue/core.py:266: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/dask_jobqueue/core.py:285: FutureWarning: env_extra has been renamed to job_script_prologue. You are still using it (even if only set to []; please also check config files). If you did not set job_script_prologue yet, env_extra will be respected for now, but it will be removed in a future release. If you already set job_script_prologue, env_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44241 instead\n",
      "  warnings.warn(\n",
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/distributed/utils.py:189: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to hostname: [Errno 101] Network is unreachable\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a SLURM cluster\n",
    "cluster = SLURMCluster(\n",
    "    cores=4,  # Number of cores per worker\n",
    "    memory=\"16GB\",  # Memory per worker\n",
    "    processes=1,  # Number of processes per worker\n",
    "    walltime=\"01:00:00\",  # Maximum runtime\n",
    "    job_extra=[\"--export=ALL\"],  # Export environment variables\n",
    "    env_extra=[\n",
    "        \"module load anconda\",  # Load necessary modules\n",
    "        \"conda activate diag\",  # Activate the Conda environment\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Scale the cluster\n",
    "cluster.scale(jobs=10)  # Request 10 workers\n",
    "\n",
    "# Connect the client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed4b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2000/M06/LS_OLv8_M36.tavg24_1d_lnd_Nt.20000601_1200z.nc4\n",
      "/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2000/M06/LS_OLv8_M36.tavg24_1d_lnd_Nt.20000602_1200z.nc4\n",
      "/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2000/M06/LS_OLv8_M36.tavg24_1d_lnd_Nt.20000603_1200z.nc4\n",
      "/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2000/M06/LS_OLv8_M36.tavg24_1d_lnd_Nt.20000604_1200z.nc4\n",
      "/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2000/M06/LS_OLv8_M36.tavg24_1d_lnd_Nt.20000605_1200z.nc4\n",
      "Loading 8401 files\n",
      "Done loading files.\n",
      "CPU times: user 3min 37s, sys: 4.48 s, total: 3min 41s\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Find all matching files using glob ---\n",
    "file_pattern = os.path.join(\n",
    "    root_directory,\n",
    "    'Y*',\n",
    "    'M*',\n",
    "    f'{expt_name}.tavg24_1d_lnd_Nt.2*.nc4'\n",
    ")\n",
    "\n",
    "all_files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# print the first 5 files, one per line\n",
    "for file in all_files[:5]:\n",
    "    print(file)\n",
    "\n",
    "# --- Parse date from filenames like:\n",
    "# snow_LS_OLv8_M36.tavg24_1d_lnd_Nt.20030101_1200z.nc4\n",
    "selected_files = []\n",
    "for file in all_files:\n",
    "    basename = os.path.basename(file)\n",
    "    try:\n",
    "        date_str = basename.split('.')[-2].split('_')[0]  # '20030101' just before the '_1200z.nc4'\n",
    "        file_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "        if start_date <= file_date <= end_date:\n",
    "            selected_files.append(file)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# --- Load all selected datasets using nested combine with explicit concat_dim ---\n",
    "print(f\"Loading {len(selected_files)} files\")\n",
    "combined_ds = xr.open_mfdataset(\n",
    "    selected_files,\n",
    "    combine='nested',\n",
    "    concat_dim='time',\n",
    "    parallel=True,  # Enable parallel processing with Dask\n",
    "    engine='netcdf4',\n",
    "    chunks={}\n",
    ")\n",
    "\n",
    "print('Done loading files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f19e59-c514-4366-9cf5-8ad3dd5f2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 409 ms, sys: 3.48 ms, total: 413 ms\n",
      "Wall time: 411 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the variables to be extracted\n",
    "variables = {\n",
    "    'precipitation_total_surface_flux': 'PRECTOTCORRLAND',\n",
    "    'snowfall_land': 'PRECSNOCORRLAND',\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results = {var: {} for var in variables}\n",
    "\n",
    "# Perform calculations for each variable\n",
    "for var, ds_var in variables.items():\n",
    "    results[var]['mean'] = combined_ds[ds_var].mean(dim='time', skipna=True)\n",
    "    results[var]['std'] = combined_ds[ds_var].std(dim='time', skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20ec64b-3690-4a68-934c-fe808769dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 28.42 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 2.7 s, total: 1min 27s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute all results in parallel\n",
    "computed_results = compute(*[results[var]['mean'] for var in variables] + \n",
    "                           [results[var]['std'] for var in variables])\n",
    "\n",
    "# Organize results back into dictionaries\n",
    "for i, var in enumerate(variables):\n",
    "    results[var]['mean'] = computed_results[i]\n",
    "    results[var]['std'] = computed_results[i + len(variables)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d077bb-8fce-46dc-be9b-6743e3ca0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.99 ms, sys: 0 ns, total: 2.99 ms\n",
      "Wall time: 84.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the statistics to a new .npz file\n",
    "np.savez(f'{expt_name}_{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_tavg24_1d_lnd_Nt_stats.npz',\n",
    "         **{f'mean_{var}': results[var]['mean'].values for var in variables},\n",
    "         **{f'std_{var}': results[var]['std'].values for var in variables})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d809b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amfox/.conda/envs/diag/lib/python3.10/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 214.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the variables to be extracted\n",
    "variables = {\n",
    "    'sm_surface': 'SFMC',\n",
    "    'sm_rootzone': 'RZMC',\n",
    "    'sm_profile': 'PRMC',\n",
    "    'precipitation_total_surface_flux': 'PRECTOTCORRLAND',\n",
    "    'vegetation_greenness_fraction': 'GRN',\n",
    "    'leaf_area_index': 'LAI',\n",
    "    'snow_mass': 'SNOMASLAND',\n",
    "    'surface_temperature_of_land_incl_snow': 'TSURFLAND',\n",
    "    'soil_temperature_layer_1': 'TSOIL1',\n",
    "    'snowfall_land': 'PRECSNOCORRLAND',\n",
    "    'snow_depth_within_snow_covered_area_fraction_on_land': 'SNODPLAND',\n",
    "    'snowpack_evaporation_latent_heat_flux_on_land': 'LHLANDSBLN',\n",
    "    'overland_runoff_including_throughflow': 'RUNSURFLAND',\n",
    "    'baseflow_flux_land': 'BASEFLOWLAND',\n",
    "    'snowmelt_flux_land': 'SMLAND',\n",
    "    'total_evaporation_land': 'EVLAND',\n",
    "    'net_shortwave_flux_land': 'SWLAND',\n",
    "    'total_water_storage_land': 'TWLAND',\n",
    "    'fractional_area_of_snow_on_land': 'FRLANDSNO'  # New variable added\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results = {var: {} for var in variables}\n",
    "\n",
    "# Perform calculations for each variable\n",
    "for var, ds_var in variables.items():\n",
    "    results[var]['concat'] = combined_ds[ds_var]\n",
    "    results[var]['mean'] = combined_ds[ds_var].mean(dim='time', skipna=True)\n",
    "    results[var]['std'] = combined_ds[ds_var].std(dim='time', skipna=True)\n",
    "\n",
    "# Compute all results in parallel\n",
    "computed_results = compute(*[results[var]['mean'] for var in variables] + \n",
    "                           [results[var]['std'] for var in variables])\n",
    "\n",
    "# Organize results back into dictionaries\n",
    "for i, var in enumerate(variables):\n",
    "    results[var]['mean'] = computed_results[i]\n",
    "    results[var]['std'] = computed_results[i + len(variables)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the concatenated variables to a new .npz file\n",
    "np.savez(f'{expt_name}_{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_tavg24_1d_lnd_Nt_concat.npz',\n",
    "         **{f'{var}_concat': results[var]['concat'].values for var in variables})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the statistics to a new .npz file\n",
    "np.savez(f'{expt_name}_{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_tavg24_1d_lnd_Nt_stats.npz',\n",
    "         **{f'mean_{var}': results[var]['mean'].values for var in variables},\n",
    "         **{f'std_{var}': results[var]['std'].values for var in variables})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98177614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the mean, etc. for each time step along the tile dimension\n",
    "ts_results = {var: {} for var in variables}\n",
    "for var in variables:\n",
    "    ts_results[var]['mean'] = combined_ds[variables[var]].mean(dim='tile', skipna=True)\n",
    "    ts_results[var]['std'] = combined_ds[variables[var]].std(dim='tile', skipna=True)\n",
    "\n",
    "# Compute all time series results in parallel\n",
    "ts_computed_results = compute(*[ts_results[var]['mean'] for var in variables] + \n",
    "                              [ts_results[var]['std'] for var in variables])\n",
    "\n",
    "# Organize time series results back into dictionaries\n",
    "for i, var in enumerate(variables):\n",
    "    ts_results[var]['mean'] = ts_computed_results[i]\n",
    "    ts_results[var]['std'] = ts_computed_results[i + len(variables)]\n",
    "\n",
    "# Save the time series to a new .npz file\n",
    "np.savez(f'{expt_name}_{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_tavg24_1d_lnd_Nt_timeseries.npz',\n",
    "         **{f'ts_mean_{var}': ts_results[var]['mean'].values for var in variables},\n",
    "         **{f'ts_std_{var}': ts_results[var]['std'].values for var in variables})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4683b3-610e-4793-b5b6-adc359bf9488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277eea02-cc73-4224-b2e2-de3db57cac85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
